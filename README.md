# Brand_Delights_Disappointment_Maps
Build Brand Delights/Disappointment maps using the Amazon Product Reviews Dataset from Kaggle. It includes objectives, data &amp; preprocessing steps, modeling &amp; visualization.

## This repo contains notebooks and scripts to:

Preprocess Amazon reviews from Kaggle.

Build TF-IDF representations and word clouds per brand.

Run LDA and K-means to extract topics and clusters.

Create a Brand Map from brand-level centroids or topic mixes.

Extract delighters and disappointers using sentiment-partitioned term scoring.
The analysis is exploratory â€” useful for hypothesis generation â€” but not a replacement for representative brand health measurement.

# ğŸ“‚ Dataset

Source: Amazon Product Reviews Dataset â€“ Kaggle

Fields: id, brand , categories , colors , dateAdded  , dateUpdated , dimension , keys , reviews.rating , reviews.text , reviews.title , reviews.userCity etc
Subset: Choose a single product category (e.g., â€œHeadphonesâ€, â€œTabletâ€) and focus on leading brands by review volume.

# ğŸ§¹ Data Preprocessing

Steps performed:

Filtering: Select top brands within a chosen product category.

Cleaning:

Convert text to lowercase

Remove special characters, punctuation, and numbers

Remove stopwords (NLTK/spaCy) and custom stop words (amazon, product, device..etc)

Lemmatization for better interpretability

Tokenization: Keep unigrams and bigrams (e.g., battery life, customer service).

Balancing: Handle brand imbalance by sampling proportionally.

Feature Generation: Build TF-IDF matrices using TfidfVectorizer with ngram_range=(1,2) and min_df=5.

â˜ï¸ Wordclouds per Brand

Generate Wordclouds for each brand based on TF-IDF frequencies.

Create separate clouds for:

All reviews

Positive reviews (rating â‰¥ 4)

Negative reviews (rating â‰¤ 2)

Compare to identify dominant discussion themes per brand.

Example insights:

Brand Aâ€™s positive word cloud emphasizes â€œsound qualityâ€ and â€œbattery life,â€ while negatives focus on â€œconnectionâ€ and â€œsupport.â€

# ğŸ§¾ Topic Modeling (LDA)

Goal: Discover latent discussion themes within reviews.

Apply Latent Dirichlet Allocation (LDA) to the TF-IDF or Count matrix.

Tune the number of topics (8â€“20) using coherence scores.

Extract and label top terms per topic.

Compute average topic weights per brand â†’ reveal brand-specific themes.

Output files:

lda_topics.csv â€” list of top words per topic

topic_distribution_per_brand.csv

# ğŸ” Document Clustering (K-Means)

Objective: Group reviews into semantic clusters using term patterns.

Apply K-Means clustering on reduced TF-IDF features (via SVD/PCA).

Extract 3â€“5 clusters and interpret each clusterâ€™s main theme.

Compare brand composition across clusters.

Output files:

cluster_summary.csv â€” key terms and representative reviews per cluster

# ğŸ—ºï¸ Brand Map Generation

Approach 1 â€“ TF-IDF Centroid Method:

Compute each brandâ€™s mean TF-IDF vector.

Apply MDS / PCA / UMAP to project into 2D.

Plot brands in a semantic space â€” nearby brands share similar review language.

Approach 2 â€“ Topic Distribution Method:

Average topic probabilities per brand.

Project via PCA to visualize topic-based brand proximity.

Output:
brand_map.png â€” a 2D brand positioning map with labels, colors by average sentiment, and sizes by review count.

# ğŸŒŸ Delighters & âš ï¸ Disappointers

Partition reviews into Positive (â‰¥4â˜…) and Negative (â‰¤2â˜…).

Compute term associations using log-odds ratios or term frequency contrasts.

Rank terms most associated with each sentiment per brand.

Interpret results:

High-scoring positive terms â†’ Delighters

High-scoring negative terms â†’ Disappointers

Output:
brand_delighters_disappointers.csv

# ğŸ“Š Evaluation Metrics

Topic coherence (LDA)

Silhouette score (K-Means)

Brand-topic distinctiveness via cosine similarity

Manual review of sample documents for interpretability
